{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n"
     ]
    }
   ],
   "source": [
    "cols = list(pd.read_csv(\"creditcard.csv\", nrows =1))\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284806, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = raw_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time        V1        V2        V3        V4        V5        V6  \\\n",
       "541    406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "623    472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4920  4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6108  6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6329  7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "541  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
       "623   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "4920  0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391  0.172726   \n",
       "6108 -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968 -0.436207   \n",
       "6329  1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181 -0.656805   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "541   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "623  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "4920 -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "6108 -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "6329 -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.loc[raw_data[\"Class\"] != 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = np.zeros([m, 25])\n",
    "for i in range(24):\n",
    "    XY[:, i] = raw_data[\"V\"+str(i+1)]\n",
    "    \n",
    "XY[:, -1] = raw_data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_indices = np.where(XY[:,-1] == 0)[0]\n",
    "flag_indices = np.where(XY[:,-1] == 1)[0]\n",
    "\n",
    "XY_clear = XY[clear_indices,:]\n",
    "XY_flag = XY[flag_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(XY_clear)\n",
    "np.random.shuffle(XY_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train = np.concatenate((XY_clear[:XY_clear.shape[0]*8//10, :], XY_flag[:XY_flag.shape[0]*8//10, :]))\n",
    "XY_test = np.concatenate((XY_clear[XY_clear.shape[0]*8//10:, :], XY_flag[XY_flag.shape[0]*8//10:, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 56962)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = XY_train[:, :-1].T\n",
    "Y_train = XY_train[:, -1].reshape((1, XY_train.shape[0]))\n",
    "X_test = XY_test[:, :-1].T\n",
    "Y_test = XY_test[:, -1].reshape((1, XY_test.shape[0]))\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[Y_test == 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None), name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, ... WL, bL\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)\n",
    "    L = len(layer_dims)\n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = tf.get_variable('W' + str(l), [layer_dims[l],layer_dims[l-1]], \\\n",
    "                                                   initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        parameters['b' + str(l)] = tf.get_variable('b' + str(l), [layer_dims[l],1], \\\n",
    "                                                   initializer = tf.zeros_initializer())\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, layer_dims):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> ... -> LOGREG\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    ZL -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "   \n",
    "    ZL = X\n",
    "    \n",
    "    for l in range(1, L-1):\n",
    "        ZL = tf.add(tf.matmul(parameters['W' + str(l)], ZL), parameters['b' + str(l)])\n",
    "        AL = tf.nn.relu(ZL)\n",
    "        \n",
    "    ZL = tf.add(tf.matmul(parameters['W' + str(L-1)], ZL), parameters['b' + str(L-1)])\n",
    "    \n",
    "    return ZL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(ZL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    ZL -- output of forward propagation (output of the last LINEAR unit),\n",
    "    Y -- \"true\" labels vector placeholder, same shape as ZL\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for the model function, including mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 if positive), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed for consistency\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    # number of mini batches of size mini_batch_size in your partitionning\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size :]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, layer_dims, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    ZL = forward_propagation(X, parameters, layer_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(ZL, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\",\n",
    "                # the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(ZL), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [24, 4, 1] #  2-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.969422\n",
      "Cost after epoch 5: 0.161329\n",
      "Cost after epoch 10: 0.022412\n",
      "Cost after epoch 15: 0.006745\n",
      "Cost after epoch 20: 0.004991\n",
      "Cost after epoch 25: 0.004504\n",
      "Cost after epoch 30: 0.004299\n",
      "Cost after epoch 35: 0.004194\n",
      "Cost after epoch 40: 0.004138\n",
      "Cost after epoch 45: 0.004101\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test, layer_dims, learning_rate = 0.00001,\n",
    "                   num_epochs = 50, minibatch_size = 32, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]%32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_for_predict(X, parameters, layer_dims):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \\\n",
    "    ZL = X\n",
    "    \n",
    "    for l in range(1, L-1):\n",
    "        ZL = np.add(np.dot(parameters['W' + str(l)], ZL), parameters['b' + str(l)])\n",
    "        AL = np.max(ZL, 0)   #Relu activation\n",
    "\n",
    "    ZL = np.add(np.dot(parameters['W' + str(L-1)], ZL), parameters['b' + str(L-1)])\n",
    "    AL = 1/(1 + np.exp(-ZL))    #Sigmoid activation\n",
    "    \n",
    "    return AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters, threshold=0.5):\n",
    "    AL = forward_propagation_for_predict(X, parameters, layer_dims)[0]\n",
    "    \n",
    "    pred = (AL > threshold).astype(int)\n",
    "    \n",
    "    tp = np.sum(pred*Y)\n",
    "    tn = np.sum((1-pred)*(1-Y))\n",
    "    fp = np.sum(pred*(1-Y))\n",
    "    fn = np.sum((1-pred)*Y)\n",
    "    \n",
    "    print(\"True positives:\", tp)\n",
    "    print(\"True negatives:\", tn)\n",
    "    print(\"False positives:\", fp)\n",
    "    print(\"False negatives:\", fn)\n",
    "    \n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    F1 = 2*precision*recall/(precision + recall)\n",
    "    \n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"F1:\", F1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 240.0\n",
      "True negatives: 227419.0\n",
      "False positives: 32.0\n",
      "False negatives: 153.0\n",
      "precision: 0.8823529411764706\n",
      "recall: 0.6106870229007634\n",
      "F1: 0.7218045112781953\n"
     ]
    }
   ],
   "source": [
    "predict(X_train, Y_train, parameters, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 70.0\n",
      "True negatives: 56854.0\n",
      "False positives: 9.0\n",
      "False negatives: 29.0\n",
      "precision: 0.8860759493670886\n",
      "recall: 0.7070707070707071\n",
      "F1: 0.7865168539325842\n"
     ]
    }
   ],
   "source": [
    "predict(X_test, Y_test, parameters, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
