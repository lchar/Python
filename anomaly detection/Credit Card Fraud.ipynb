{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n"
     ]
    }
   ],
   "source": [
    "cols = list(pd.read_csv(\"creditcard.csv\", nrows =1))\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284806, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = raw_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time        V1        V2        V3        V4        V5        V6  \\\n",
       "541    406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "623    472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4920  4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6108  6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6329  7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "\n",
       "            V7        V8        V9  ...         V21       V22       V23  \\\n",
       "541  -2.537387  1.391657 -2.770089  ...    0.517232 -0.035049 -0.465211   \n",
       "623   0.325574 -0.067794 -0.270953  ...    0.661696  0.435477  1.375966   \n",
       "4920  0.562320 -0.399147 -0.238253  ...   -0.294166 -0.932391  0.172726   \n",
       "6108 -3.496197 -0.248778 -0.247768  ...    0.573574  0.176968 -0.436207   \n",
       "6329  1.713445 -0.496358 -1.282858  ...   -0.379068 -0.704181 -0.656805   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "541   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "623  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "4920 -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "6108 -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "6329 -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.loc[raw_data[\"Class\"] != 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = np.zeros([m, 25])\n",
    "for i in range(24):\n",
    "    XY[:, i] = raw_data[\"V\"+str(i+1)]\n",
    "    \n",
    "XY[:, -1] = raw_data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_indices = np.where(XY[:,-1] == 0)[0]\n",
    "flag_indices = np.where(XY[:,-1] == 1)[0]\n",
    "\n",
    "XY_clear = XY[clear_indices,:]\n",
    "XY_flag = XY[flag_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(XY_clear)\n",
    "np.random.shuffle(XY_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train = np.concatenate((XY_clear[:XY_clear.shape[0]*8//10, :], XY_flag[:XY_flag.shape[0]*8//10, :]))\n",
    "XY_test = np.concatenate((XY_clear[XY_clear.shape[0]*8//10:, :], XY_flag[XY_flag.shape[0]*8//10:, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 56962)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = XY_train[:, :-1].T\n",
    "Y_train = XY_train[:, -1].reshape((1, XY_train.shape[0]))\n",
    "X_test = XY_test[:, :-1].T\n",
    "Y_test = XY_test[:, -1].reshape((1, XY_test.shape[0]))\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[Y_test == 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "    \n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, weight):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = -1/m*np.sum(weight*Y*np.log(AL) + (1 - Y)*np.log(1 - AL))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "    db = 1/m*np.sum(dZ, axis = 1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1 + np.exp(-Z))\n",
    "    dZ = dA*s*(1-s)\n",
    "    \n",
    "    assert(dZ.shape == dA.shape)\n",
    "    \n",
    "    return dZ\n",
    "    \n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    dZ = dA*np.heaviside(Z, 0)\n",
    "    \n",
    "    assert(dZ.shape == dA.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, weight):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (weight*np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, weight, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (â‰ˆ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "        cost = compute_cost(AL, Y, weight)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches, weight)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [24, 4, 1] #  2-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.703885\n",
      "Cost after iteration 100: 0.152234\n",
      "Cost after iteration 200: 0.108541\n",
      "Cost after iteration 300: 0.094833\n",
      "Cost after iteration 400: 0.088008\n",
      "Cost after iteration 500: 0.083592\n",
      "Cost after iteration 600: 0.080225\n",
      "Cost after iteration 700: 0.077398\n",
      "Cost after iteration 800: 0.074884\n",
      "Cost after iteration 900: 0.072570\n",
      "Cost after iteration 1000: 0.070396\n",
      "Cost after iteration 1100: 0.068326\n",
      "Cost after iteration 1200: 0.066339\n",
      "Cost after iteration 1300: 0.064417\n",
      "Cost after iteration 1400: 0.062548\n",
      "Cost after iteration 1500: 0.060720\n",
      "Cost after iteration 1600: 0.058924\n",
      "Cost after iteration 1700: 0.057160\n",
      "Cost after iteration 1800: 0.055428\n",
      "Cost after iteration 1900: 0.053725\n",
      "Cost after iteration 2000: 0.052052\n",
      "Cost after iteration 2100: 0.050409\n",
      "Cost after iteration 2200: 0.048803\n",
      "Cost after iteration 2300: 0.047236\n",
      "Cost after iteration 2400: 0.045708\n",
      "Cost after iteration 2500: 0.044221\n",
      "Cost after iteration 2600: 0.042776\n",
      "Cost after iteration 2700: 0.041377\n",
      "Cost after iteration 2800: 0.040025\n",
      "Cost after iteration 2900: 0.038720\n",
      "Cost after iteration 3000: 0.037461\n",
      "Cost after iteration 3100: 0.036248\n",
      "Cost after iteration 3200: 0.035086\n",
      "Cost after iteration 3300: 0.033974\n",
      "Cost after iteration 3400: 0.032910\n",
      "Cost after iteration 3500: 0.031897\n",
      "Cost after iteration 3600: 0.030931\n",
      "Cost after iteration 3700: 0.030014\n",
      "Cost after iteration 3800: 0.029149\n",
      "Cost after iteration 3900: 0.028331\n",
      "Cost after iteration 4000: 0.027560\n",
      "Cost after iteration 4100: 0.026829\n",
      "Cost after iteration 4200: 0.026139\n",
      "Cost after iteration 4300: 0.025489\n",
      "Cost after iteration 4400: 0.024876\n",
      "Cost after iteration 4500: 0.024298\n",
      "Cost after iteration 4600: 0.023760\n",
      "Cost after iteration 4700: 0.023254\n",
      "Cost after iteration 4800: 0.022781\n",
      "Cost after iteration 4900: 0.022336\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, Y_train, layers_dims, weight = 10, learning_rate = 0.1, num_iterations = 5000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters, threshold):\n",
    "    AL = L_model_forward(X, parameters)[0]\n",
    "    \n",
    "    pred = (AL > threshold).astype(int)\n",
    "    \n",
    "    tp = np.sum(pred*Y)\n",
    "    tn = np.sum((1-pred)*(1-Y))\n",
    "    fp = np.sum(pred*(1-Y))\n",
    "    fn = np.sum((1-pred)*Y)\n",
    "    \n",
    "    print(\"True positives:\", tp)\n",
    "    print(\"True negatives:\", tn)\n",
    "    print(\"False positives:\", fp)\n",
    "    print(\"False negatives:\", fn)\n",
    "    \n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    F1 = 2*precision*recall/(precision + recall)\n",
    "    \n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"F1:\", F1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL = L_model_forward(X_train, parameters)[0]\n",
    "pred = (AL > 0.7).astype(int)\n",
    "np.sum((1-pred)*Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 323.0\n",
      "True negatives: 227355.0\n",
      "False positives: 96.0\n",
      "False negatives: 70.0\n",
      "precision: 0.7708830548926014\n",
      "recall: 0.821882951653944\n",
      "F1: 0.7955665024630542\n"
     ]
    }
   ],
   "source": [
    "predict(X_train, Y_train, parameters, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 83.0\n",
      "True negatives: 56845.0\n",
      "False positives: 18.0\n",
      "False negatives: 16.0\n",
      "precision: 0.8217821782178217\n",
      "recall: 0.8383838383838383\n",
      "F1: 0.83\n"
     ]
    }
   ],
   "source": [
    "predict(X_test, Y_test, parameters, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
